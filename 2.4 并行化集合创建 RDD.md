## 并行化集合创建 RDD
### 并行化集合创建 RDD（Java）

```
package cn.spark.study.core;

import java.util.Arrays;
import java.util.List;

import org.apache.spark.SparkConf;
import org.apache.spark.api.java.JavaRDD;
import org.apache.spark.api.java.JavaSparkContext;
import org.apache.spark.api.java.function.Function2;

public class ParallelizeCollection {

	public static void main(String[] args) {
		// 创建 SaprkConf
		SparkConf conf = new SparkConf().setAppName("ParallelizeCollection").setMaster("local");

		// 创建 JavaSparkContext
		JavaSparkContext context = new JavaSparkContext(conf);

		// 通过并行化集合方式创建 RDD，调用 SparkContext 及其子类的 parallelize()
		List<Integer> numbers = Arrays.asList(1, 2, 3, 4, 5, 6, 7, 8, 9, 10);
		JavaRDD<Integer> numbersRDD = context.parallelize(numbers);

		// 执行 reduce 算子操作，相当于先进行 1+2=3，再用 3+3=6，然后 6+4=10
		int sum = numbersRDD.reduce(new Function2<Integer, Integer, Integer>() {

			@Override
			public Integer call(Integer v1, Integer v2) throws Exception {
				return v1 + v2;
			}
		});
		
		//输出累计的和
		System.out.println(sum);
		//关闭 JavaSparkContext
		context.close();
	}
}

```
### 并行化集合创建 RDD（scala）

```
package cn.spark.study.core

import com.sun.tools.javac.code.Attribute.Array
import org.apache.spark.SparkConf
import org.apache.spark.SparkContext

object ParallelizeCollection {
    def main(args: Array[String]) {
        val conf = new Sparkconf()
        .setAppName("ParallelizeCollection")
        .setMaster("local")
        
        val context = new SaprkContext(conf)
        val numbers = Array(1,2,3,4,5,6,7,8,9,10)
        val rdd = context.parallelize(numbers)
        val sum = rdd.reduce(_+_)
        println(sum)
    }
}
```
