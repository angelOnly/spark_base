## 创建 RDD （本地文件、HDFS 文件）
### 创建 RDD
在进行 Spark 核心编程的时候，首先要创建 RDD。

该 RDD 中通常代表和包含了 Spark 应用程序的输入源数据。

然后在创建了 RDD 之后，才可以通过 Spark Core 提供的 transformation 算子对该 RDD 进行转换，来获取其它 RDD。

>**Saprk Core 提供三种创建 RDD 的方式**
1. 使用程序中的集合创建 RDD
 
    主要用于进行测试，可在实际部署到集群运行之前，自己使用集合构造测试数据，来测试后面的 Spark 应用的流程。
2. 使用本地文件创建 RDD
    
    主要用于临时性处理一些存储大量数据的文件
3. 使用 HDFS 创建 RDD
    
    最常用的生产环境处理方式，主要可以针对 HDFS 上存储的大数据进行离线批处理操作。

### 使用本地文件和 HDFS 创建 RDD
Spark 支持使用任何 Hadoop 支持的存储系统上的文件创建 RDD，比如 HDFS，Cassandra，HBase 及本地文件。

通过调用 SparkContext 的 textfile() 可以针对文件或 HDFS 文件创建 RDD。

>**注意事项**
1. 如果针对本地文件，在本地测试，只需要本地有一份文件即可，如果是 Spark 集群上针对 linux 本地文件，需要将文件拷贝到所有 worker 节点上
2. Spark 的 textfile 方法支持针对目录、压缩文件以及通配符进行 RDD 创建
3. Spark 默认会为 hdfs 文件的每一个 block 创建一个 partition，但也可以通过 textfile 第二个参数手动设置分区数量，只能比 block 数量多，不能比 block 数量少。


```
val rdd = sc.textfile("data.txt")
val wordCount = rdd.map(line => line.length).reduce(_+_)
```
#### HDFS 创建 RDD (java)

```
package cn.spark.study.core;

import org.apache.spark.SparkConf;
import org.apache.spark.api.java.JavaRDD;
import org.apache.spark.api.java.JavaSparkContext;
import org.apache.spark.api.java.function.Function;
import org.apache.spark.api.java.function.Function2;

public class CountNumHDFSFile {

	public static void main(String[] args) {

		SparkConf conf = new SparkConf().setAppName("CountNumLocalFile");
		JavaSparkContext sc = new JavaSparkContext(conf);

		JavaRDD<String> rdd = sc.textFile("hdfs://spark1:9000/spark.txt");

		JavaRDD<Integer> lenRDD = rdd.map(new Function<String, Integer>() {

			@Override
			public Integer call(String v1) throws Exception {
				return v1.length();
			}
		});

		int count = lenRDD.reduce(new Function2<Integer, Integer, Integer>() {

			@Override
			public Integer call(Integer v1, Integer v2) throws Exception {
				return v1 + v2;
			}
		});
		
		System.out.println(count);
	}

}

```
