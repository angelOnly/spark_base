## Saprk 实现 Java 版 wordCount

创建的是 maven project，并对配置文件进行配置。

#### 1. 创建 SaprkConf 对象

- setMaster：设置 spark 要连接的 spark 集群的 master 节点的 url。local 表示在本地运行。

````java
SparkConf conf = new SparkConf()
				.setAppName("WordCountLocal")
				.setMaster("local");  
````

#### 2. 创建 JavaSparkContext 对象

- SparkContext 是 spark 所有功能的一个入口，无论是 java，scala，python，都必须有一个 SparkContext。
- 主要作用：初始化 spark 中一些所需的核心组件。包括调度器（DAGScheduler，TaskScheduler），还会到 Spark Master 节点上进行注册。
- SparkContext 在不同的类型中不同。
  - java：JavaSaprkContext
  - Saprk SQL：SQLContext，HiveContext
  - Spark Streaming：SparkContext

````java
JavaSparkContext sc = new JavaSparkContext(conf);
````

### 3. 针对输入源（hdfs文件，本地文件）创建一个初始 RDD

- 输入源中数据会被打散，分配到 RDD 每个 partition 中，形成一个初始分布式数据集
- textFile()：SparkContext 中用于根据文件类型输入源创建 RDD 的方法
- JavaRDD：java 中创建的普通 RDD
- RDD 中有元素的概念，如果是 hdfs 或本地文件，创建的 RDD，每个元素相当于文件中的一行

````java
JavaRDD<String> lines = sc.textFile("spark.txt");
````

#### 4. 对初始 RDD 进行 transformation 操作

- 通常操作会通过 function，配合 RDD 的 map，flatmap 等算子进行

- function：如果比较简单，则创建指定的 Function 匿名内部类。

  如果较复杂，会单独创建一个类，作为实现该 function 的接口的类。

- 先将一行拆分成单个单词，在本例中，输入是 String（一行一行的文本），输出也是 String（每一行的文本）

- FlatMapFunction<String, String>：两个泛型参数代表输入和输出

- flatmap 算子：将 RDD 的一个元素拆分成一个或多个元素

````java
JavaRDD<String> words = lines.flatMap(new FlatMapFunction<String, String>() {
	private static final long serialVersionUID = 1L;
    @Override
    public Iterable<String> call(String line) throws Exception {
        return Arrays.asList(line.split(" "));  
    }
});
````

#### 5. 将每个单词映射为 （单词，1）这种格式

- 后面会根据单词为 key，来进行每个单词出现次数的累加
- mapToPair：将每个元素映射为 (v1, v2) 的 tuple2。
- mapToPair 与 PairFunction<String, String, Integer> 配合使用
  - 泛型1：输入类型
  - 泛型2： tuple2 的第一个值类型
  - 泛型3： tuple2 的第二个值类型
- JavaPairRDD<String, Integer>：代表 tuple 中第一个元素和第二个元素

````java
JavaPairRDD<String, Integer> pairs = words.mapToPair(
new PairFunction<String, String, Integer>() {
    private static final long serialVersionUID = 1L;
    @Override
    public Tuple2<String, Integer> call(String word) throws Exception {
        return new Tuple2<String, Integer>(word, 1);
    }
});
````

#### 6. 以单词作为 key，统计每个单词出现的次数

- reduceByKey：对每个 key 对应的 value，都进行 reduce 操作。
  - 如：JavaPairRDD 中有几个元素，分别为 (hello, 1) (hello, 1) (hello, 1) (world, 1)，reduce操作，相当于是把第一个值和第二个值进行计算，然后再将结果与第三个值进行计算。
  - hello，那么就相当于是，首先是 1 + 1 = 2，然后再将 2 + 1 = 3。
  - 最后返回的 JavaPairRDD 中的元素，也是 tuple，第一个值就是每个 key，第二个值就是 key 的 value。
  - reduce 之后的结果，相当于就是每个单词出现的次数。

````java
JavaPairRDD<String, Integer> wordCounts = pairs.reduceByKey(
new Function2<Integer, Integer, Integer>() {
    private static final long serialVersionUID = 1L;
    @Override
    public Integer call(Integer v1, Integer v2) throws Exception {
        return v1 + v2;
    }
});
````

#### 7. 对以上的 transformation 操作进行 action 操作

- 通过以上的 spark 算子（flatmap，mapToPair，reduceByKey）操作已经统计出单词次数，但在 spark 中，光有 transformation 是不会执行的，还需要 action 操作。
- 这里的一个 action 操作为 foreach 来触发程序执行

````java
wordCounts.foreach(new VoidFunction<Tuple2<String,Integer>>() {
	private static final long serialVersionUID = 1L;
    @Override
    public void call(Tuple2<String, Integer> wordCount) throws Exception {
        System.out.println(wordCount._1 + " appeared " + wordCount._2 + " times.");    
    }
});
sc.close();
````

#### 全部代码

1. 创建 SparkConf 对象，设置 Spark 配置信息
2. 创建 SparkContext 对象，初始化 Spark 核心组件
3. 创建 RDD，获取输入源
4. 将 RDD 进行 flatMap 进行展开
5. 将展开的 RDD 进行 tuple 组合操作
6. 对 tuple 进行统计
7. 执行（action）以上 transformation 操作

````java
package cn.spark.study.core;
import java.util.Arrays;
import org.apache.spark.SparkConf;
import org.apache.spark.api.java.JavaPairRDD;
import org.apache.spark.api.java.JavaRDD;
import org.apache.spark.api.java.JavaSparkContext;
import org.apache.spark.api.java.function.FlatMapFunction;
import org.apache.spark.api.java.function.Function2;
import org.apache.spark.api.java.function.PairFunction;
import org.apache.spark.api.java.function.VoidFunction;
import scala.Tuple2;
/**
 * 使用java开发本地测试的wordcount程序
 */
public class WordCountLocal {
	public static void main(String[] args) {
		// 第一步：创建SparkConf对象，设置Spark应用的配置信息
		SparkConf conf = new SparkConf()
				.setAppName("WordCountLocal")
				.setMaster("local");  
		
		// 第二步：创建JavaSparkContext对象
		JavaSparkContext sc = new JavaSparkContext(conf);
		// 第三步：要针对输入源（hdfs文件、本地文件，等等），创建一个初始的RDD
		JavaRDD<String> lines = sc.textFile("spark.txt");
		// 第四步：对初始RDD进行transformation操作，也就是一些计算操作
		JavaRDD<String> words = lines.flatMap(new FlatMapFunction<String, String>(){
			private static final long serialVersionUID = 1L;
			@Override
			public Iterable<String> call(String line) throws Exception {
				return Arrays.asList(line.split(" "));  
			}
			
		});
		// 第五步：将每一个单词，映射为(单词, 1)的这种格式
		JavaPairRDD<String, Integer> pairs = words.mapToPair(new PairFunction<String, String, Integer>() {
            private static final long serialVersionUID = 1L;
            @Override
            public Tuple2<String, Integer> call(String word) throws Exception {
                return new Tuple2<String, Integer>(word, 1);
            }
		});
		// 第六步：以单词作为key，统计每个单词出现的次数
		JavaPairRDD<String, Integer> wordCounts = pairs.reduceByKey(
				new Function2<Integer, Integer, Integer>() {
					private static final long serialVersionUID = 1L;
					@Override
					public Integer call(Integer v1, Integer v2) throws Exception {
						return v1 + v2;
					}
				});
		// 第七步：对 transformation 操作进行 action 执行程序
		wordCounts.foreach(new VoidFunction<Tuple2<String,Integer>>() {
			private static final long serialVersionUID = 1L;
			@Override
			public void call(Tuple2<String, Integer> wordCount) throws Exception {
				System.out.print("("+wordCount._1 +","+wordCount._2 +"), ");
			}
		});
		sc.close();
	}
}
````

#### 运行结果

````
((MLlib),,1), (For,2), (Product,1), (it,3), (operators,1), (sits,1), (Hadoop��s,1), (have,3), (tweet,1), (stack.,1), (modification,1), (conference.,1), (we,2), (requiring,1), (This,1), (simple,1), (manager.,1), (software,2), (any,1), (make,1), (implementation,1), (seconds,1), (&,1), (out,1), (Data,1), (engine,1), (directions,1), (month,1), (the,7), (technology,3), (2.,1), (Alpine,,1), (We��d,1), (box.,1), (100X,1), (most,1), (build,1), (love,1), (be,1), (��a,1), (Apache,1), (At,1), (Alpine,1), (our,5), (including,1), (as,1), (us?,1), (dead,1), (iterative,1), (leverage,2), (Want,1), (File,1), (programming,1), (account,1), (recently,1), (engines,1), (is,2), (Horwitz,1), (on,3), (features,1), (pre-installation,1), (speed,1), (at,3), (using,1), (convenience.,1), (top,1), (integrates,1), (meaning,1), (customers,1), (new,3), (We,2), (Python.,1), (Random,1), (launch,1), (processing.��,1), (set,1), (has,4), (NextGen),1), (world,1), (Learning,1), (seamless,1), (Director,1), (generality,1), (or,4), (Yarn,1), (Java,,1), (appointment,1), (As,1), (YARN,1), (Machine,1), (company,,1), (installed,1), (50,2), (see,1), (of,10), (cluster,4), (three,1), (analytic,1), (Or,1), (Forests,,1), (rows,1), (millions,1), (rows.,1), (Hadoop,4), (characterized,1), (Spark,10), (integration,1), (job,1), (native,1), (greatest,1), (general,1), (Million,1), (extensive,1), (here,1), (big,1), (Joel,1), (1.,1), (send,1), ((HDFS),1), (3.,1), (without,2), (for,3), (models.,1), (require,1), (just,1), (@JSHorwitz.,1), (Labs,1), (latest,2), (regression,,1), (node,1), (coming,1), (your,1), (up,2), (analysis,1), (20,1), (advanced,2), (Distributed,1), (no,1), (large-scale,1), (since,1), (started,1), (empowers,1), (transformation,,1), (by,1), (like,1), (compatibility,1), (2005.,1), (both,1), (an,1), (streaming,1), ((Shark),,1), (analyzed,1), (Streaming),1), (made,1), (nicely,1), (configuration.,1), (with,3), (algorithms,,1), (meet,1), (data,4), (interesting,1), (in,5), (logistic,1), (GigaOM,1), (Summit.,1), (increase,1), (hundreds,1), (support,1), (scales,1), (Click,1), (building,1), (other,1), (course,1), (exploration,,1), (rock,1), (key,1), (you,1), (hardware,1), (that,4), (a,3), (fast,1), (their,1), (example,,1), (last,1), (SQL,1), (demonstrated,1), (will,1), (to,10), (get,1), (platform,1), (,7), (languages,1), (list,1), (there!,1), ((Spark,1), (Scala,,1), (and,7), (Marketing,1), ((Hadoop,1), (certified,1), (additional,1), (System,1)
````